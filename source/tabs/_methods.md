# Methods

I quantified what happened in the DH panels I attended for which there are Storifies (courtesy of [Eileen Clancy](https://twitter.com/clancynewyork)): #s280, #s411, and #s508. The first session, #s280, "Disrupting the Digital Humanities" (hereafter "the disrupting panel"), was led by Jesse Stommel and featured Rick Godden, Jonathan Hsy, Spencer Keralis, Eunsong Kim, Angel Nieves, Annemarie Pérez, and Jentery Sayers. (See the [Storify here](https://storify.com/clancynewyork/disrupting-dh) and the panelists' [position papers here](http://www.disruptingdh.com/category/2016-position-paper/).) The second session, #s411, "Digital Scholarship in Action: Pedagogy" (hereafter "the pedagogy panel") was led by Marguerite Helmers and Daniel Powell and featured Amy Earhart, Aaron Mauro, Kimberley R. D. McLean-Fiander, Philippa Schwarzkopf, Angel Nieves, and Jacqueline Wernimont. (See the [Storify here](https://storify.com/clancynewyork/digital-scholarship-in-action-pedagogy).) The third session, #s508, "Care and Repair: Designing Digital Scholarship" (hereafter "the repair panel"), was led by Jetery Sayers and featured Lauren Klein, Daniel Anderson, Lisa Marie Rhody, and Susan Brown. (See the [Storify here](https://storify.com/clancynewyork/care-and-repair-designing-digital-scholarship) and the panelists' [abstracts here](http://jentery.github.io/careRepair/).)

No other panels’ feeds were incorporated because it was necessary to have been there, firstly, to sort tweets into categories accurately (for instance, does a tweet record what is being said in a presentation, or does it add some sort of new commentary, idea, or critique?), and secondly, to make the difficult judgment calls required to sort presenters and Tweeters into a gender binary (which I discuss further in the Theory section). This resulted in a set of 792 tweets: a tiny sample size compared to other research projects focusing on Twitter. I maintain that this small sample set minimized the number of mistakes in sorting tweets into topical categories and gendered categories; I double-checked, and in some cases triple- or quadruple-checked, each classificatory decision I made, which took quite a long time. (I even hand-checked every single statistic generated by my spreadsheet to confirm the accuracy of my Excel equations.) More importantly, though, this case study can be seen as taking part in a broader reevaluation of the dominance of "big data" in the digital humanities, as I argue in the Theory section.

When I reached out for permission to use her Storifies, [Clancy](https://twitter.com/clancynewyork) pointed out that some super-tweeters might skew the results. I classify as **casual tweeters** those who did not regularly tweet (more specifically, they tweeted at a lower rate than one tweet per presentation), while **active tweeters** tweet at least once per presentation, and **super-tweeters** are only those who tweet continuously, producing tweets at such a markedly higher rate than active tweeters that they clearly dominate the panel's Twitter feed. In #s280 (the disrupting panel) and #s508 (the repair panel), each super-tweeter produced 31.50% and 31.46% of that panel's total tweets, respectively.

Given the not-fully-understood impact of super-tweeters (though I do reflect on this issue in the Results section), and given my small sample size, I cannot draw firm conclusions about “all” male and female academics on Twitter. But I am not concerned here with essentializing gendered tweeting habits---for, as I show, they are highly contextualized---beyond the scope of my data. And I am less concerned with individuals than with the _record as it stands_, which includes both of the super-tweeters in my sample.

I scoured the Storifies and recorded two sets of data: the first being the gender of all people involved in the tweet, including the tweeter and anyone named in the tweet, and the second being the topic or purpose of the tweet. For the first set of data (the gender of those involved), the categories I tracked included tweeters, presenters, non-presenters (typically, audience members and those not present in the room), and “no gender identified.” Tweets from organizations were not counted because their accounts did not emphasize gendered markers in their avatars or profiles. When more than one person is mentioned in a tweet, I defaulted to the first person mentioned because I did not want each tweet to count more than any other tweet. And because this case study is recording the rhetoric embedded in each tweet through structure and diction, to determine the tweet's primary interlocutor, I examined the grammar, punctuation, syntax, and other markers of emphasis in tweets that mention more than one person.

When a presenter and a non-presenter are mentioned, I counted the non-presenter because the primary purpose of such tweets is to emphasize this new person, even though the tweeter is also ensuring that the presenter would still be included in the conversation. Moderators and conference organizers counted as presenters if they made extended remarks to theorize the panel (something far more substantive than author biographies) and were _visually present_ (seated literally at the table, close to and alongside the presenters), but as non-presenters if they did not make remarks beyond presenter biographies and sat with the audience or in a corner, away from the panelist's table. To avoid mislabeling a participant, if a person was invoked but not actually named, the tweet counted as “Person mentioned but not named,” unless the gender of the person invoked was explicitly indicated in that particular tweet through pronoun usage. 

I also compared what I call the "expected versus the actual number of tweets" that cover the work of either male or female presenters. In the panels I track, there were 9 female presenters and 8 male presenters, so I assume that we could "expect," in a gender-neutral Twitter stream, 9/17 of the tweets that name presenters to be about female presenters, whereas 8/17 would be about male presenters. I then compared the _actual_ number of tweets generated when male or female scholars presented to see if either gender was overrepresented or underrepresented in comparison with the expected 9:8 ratio.

The second data set collected was the topic---more accurately, the rhetorical function---of each tweet. I devised the following categorical schema: meta-comment, summary/quotation, citation, response/question, and room climate. **Meta-comment tweets** refer to the objective state of the panel (has it begun, who is speaking, what are the paper titles, who can or cannot attend), whereas **room climate tweets** comment on the atmosphere of the room (big, small, crowded, loud, empty, cold, hot). Both categories point to the event-ness of the panel, rather than its intellectual content, but meta-commentary tweets provide the same sort of information that the program gives (only given in real time), compared to the room climate tweets' social focus on characterizing the panel's spatial, material, or emotional atmosphere (thereby narrating the affective or physical experience of being present at the panel).

**Summary or quotation tweets** record what was said by panelists, or by audience members during Q&A, with an aim to reproducing accurately what is going on. Such tweets are reportage and make up the majority of "carework" tweets. By contrast, **response or question tweets** are evaluative in nature; they judge, reflect, and editorialize on the intellectual content of the panel so that the tweeter participates actively in the conversation. These tweets often ask panelists questions about their ideas, engage in conversation with other audience members, or bringing in an absent colleague into the Twitter stream. Such tweets are transformative or interactive rather than summative or mimetic. Summary/quotation tweets are essentially focused on the panelists' materials, whereas response/question tweets are focused on the audience's responses.

**Citation tweets** provide citations, giving URLs leading to work by the panelists; to books and articles mentioned by the panelists; or to work that the tweeter considers relevant but had not been invoked by the panelists. These tweets are outward-facing, unlike meta-comment or room climate tweets; they direct the reader to sources, scholars, or moments outside the room. Like summary/quotation tweets, Citation tweets perform important labor for the panel by spontaneously constructing a bibliography. These tweets can be profoundly political or interventionist in that they have a unique power (among these categories) to _give credit_. At other times, these tweets can be more stenographical in nature by offering further information about scholarship cited by presenters.

A few tweets might technically have counted in more than one category. I judged which category by counting the number of words that reflected the two categories and classifying it in the one with the greatest number. If the balance between the number of words fitting two categories was roughly equal, I classified the tweet according to which came first; for example, a tweet that begin with strong evaluative language and expressive punctuation (such as multiple exclamation marks), but then summarized the presenter’s words, was counted as response/question, not summary/quotation.
