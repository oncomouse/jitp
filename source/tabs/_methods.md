# Methods

I quantified what happened in the DH panels I attended for which there are Storifies (courtesy of [Eileen Clancy](https://twitter.com/clancynewyork)): #s280, #s411, and #s508. The first session, #s280, "Disrupting the Digital Humanities" (hereafter "the disrupting panel"), was led by Jesse Stommel and featured Rick Godden, Jonathan Hsy, Spencer Keralis, Eunsong Kim, Angel Nieves, Annemarie Pérez, and Jentery Sayers. (See the [Storify here](https://storify.com/clancynewyork/disrupting-dh) and the panelists' [position papers here](http://www.disruptingdh.com/category/2016-position-paper/).) The second session, #s411, "Digital Scholarship in Action: Pedagogy" (hereafter "The pedagogy panel") was led by Marguerite Helmers and Daniel Powell and featured Amy Earhart, Aaron Mauro, Kimberley R. D. McLean-Fiander, Philippa Schwarzkopf, Angel Nieves, and Jacqueline Wernimont. (See the [Storify here](https://storify.com/clancynewyork/digital-scholarship-in-action-pedagogy).) The third session, #s508, "Care and Repair: Designing Digital Scholarship" (hereafter "the repair panel"), was led by Jetery Sayers and featured Lauren Klein, Daniel Anderson, Lisa Marie Rhody, and Susan Brown. (See the [Storify here](https://storify.com/clancynewyork/care-and-repair-designing-digital-scholarship) and the panelists' [abstracts here](http://jentery.github.io/careRepair/).)

No other panels’ feeds were incorporated because it was necessary to have been there, firstly, to sort tweets into categories accurately, and secondly, to make the difficult judgment calls required to sort presenters and Tweeters into a gender binary (which I discuss further in the Theory section). This resulted in a small set of 792 tweets: a tiny sample size compared to other research projects focusing on Twitter. I maintain that this small sample set minimized the number of mistakes in sorting tweets into topical categories and gendered categories; I double-checked, and in some cases triple- or quadruple-checked, each classificatory decision I made, which took quite a bit of time. More importantly, though, this case study can be seen as taking part in a broader reevaluation of the dominance of "big data" in the digital humanities, as I argue in the Theory section.

When I reached out for permission to use her Storifies, [Clancy](https://twitter.com/clancynewyork) pointed out that some super-tweeters might skew the results. I classify as **casual tweeters** those who did not regularly tweet (more specifically, they tweeted at a lower rate than one tweet per presentation), while **active tweeters** tweet at least once per presentation, and **super-tweeters** are only those who tweet continuously, producing tweets at such a markedly higher rate than active tweeters that they clearly dominate the panel's Twitter feed. In #s280 (Disrupting the Digital Humanities) and #s508 (Care and Repair), the super-tweeter produced 31.50% and 31.46% of that panel's total tweets, respectively.

Given the not-fully-understood impact of super-tweeters (though I do reflect on this issue in the Results section), and given my teensy sample size, I cannot draw firm conclusions about “all” male and female academics on Twitter. But I am not concerned here with essentializing gendered tweeting habits---for, as I show, they are highly contextualized---beyond the scope of my data. And I am less concerned with individuals than with the _record as it stands_, which includes both of the super-tweeters in my sample (one in #s280, the other in #s508). 

Two sets of data were tracked, the first being the gender of people mentioned in the tweet. Categories included tweeters, presenters, non-presenters (typically, audience members and those not present in the room), and “no male/female identified.” Tweets from organizations were not counted because their accounts did not emphasize gendered markers in their avatars or profiles. When more than one person is mentioned in a tweet, I defaulted to the first person mentioned not only to avoid tweets double-counting, but also because this case study is recording the rhetoric embedded in each tweet through structure and emphasis. 

However, when a presenter and a non-presenter are mentioned, I counted the non-presenter because such tweets emphasize this new person while making sure the presenter would still be tagged. Moderators and conference organizers counted as presenters if they made extended remarks to theorize the panel (something far more substantive than author biographies) and were _visually present_ (at the table beside presenters), but as non-presenters if they did not make remarks beyond presenter biographies and sat with the audience. To avoid mislabeling a participant, if a person was invoked but not actually named, the tweet counted as “no male/female identified” unless the gender of the person invoked was explicitly indicated in that particular tweet (e.g., through pronouns). 

The second data set collected was the topic---more accurately, the rhetorical function---of each tweet: meta-comment, summary/quotation, citation, response/question, and room climate. **Meta-comment tweets** refer to the objective state of the panel (has it begun, who is speaking, what are the paper titles, who can or cannot attend), whereas **room climate tweets** comment on the atmosphere of the room (big, small, crowded, loud, empty, cold, hot). Both categories point to the event-ness of the panel, rather than its intellectual content.

**Summary or quotation tweets** record what was said by panelists, or by audience members during Q&A, with an aim to reproducing accurately what is going on. Such tweets are reportage and make up the majority of "carework" tweets. By contrast, **response or question tweets** are evaluative in nature; they judge, reflect, and editorialize so that the tweeter participates actively in the conversation. These tweets often ask panelists questions about their ideas, engage in conversation with other audience members, or bringing in an absent colleague into the Twitter stream. Such tweets are transformative rather than summative.

**Citation tweets** provide citations, giving URLs leading to work by the panelists; to books and articles mentioned by the panelists; or to work that the tweeter considers relevant but had not been invoked by the panelists.  These tweets are outward-facing, unlike meta-comment or room climate tweets; they direct the reader to sources, scholars, or moments outside the room. Like Summary/quotation tweets, Citation tweets perform important labor for the panel by spontaneously constructing a bibliography.

A few tweets might technically have counted in more than one category. I judged which category by counting the number of words that reflected the two categories and classifying it in the one with the greatest number. If the balance between the number of words fitting two categories was equal, I classified the tweet according to the position of each category; for example, a tweet that begin with strong evaluative language and expressive punctuation (such as multiple exclamation marks), but then summarized the presenter’s words, was counted as Response/question, not Summary/quotation.
